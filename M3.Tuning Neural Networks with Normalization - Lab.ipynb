{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Neural Networks with Normalization - Lab \n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab you'll build a neural network to perform a regression task.\n",
    "\n",
    "It is worth noting that getting regression to work with neural networks can be comparatively difficult because the output is unbounded ($\\hat y$ can technically range from $-\\infty$ to $+\\infty$), and the models are especially prone to exploding gradients. This issue makes a regression exercise the perfect learning case for tinkering with normalization and optimization strategies to ensure proper convergence!\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this lab you will: \n",
    "\n",
    "- Fit a neural network to normalized data \n",
    "- Implement and observe the impact of various initialization techniques \n",
    "- Implement and observe the impact of various optimization techniques \n",
    "\n",
    "## Load the data \n",
    "\n",
    "First, run the following cell to import all the neccessary libraries and classes you will need in this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary libraries and classes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras import initializers\n",
    "from keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you'll be working with the housing prices data you saw in an earlier section. However, we did a lot of preprocessing for you so you can focus on normalizing numeric features and building neural network models! The following preprocessing steps were taken (all the code can be found in the `data_preprocessing.ipynb` notebook in this repository): \n",
    "\n",
    "- The data was split into the training, validate, and test sets \n",
    "- All the missing values in numeric columns were replaced by the median of those columns \n",
    "- All the missing values in catetgorical columns were replaced with the word 'missing' \n",
    "- All the categorical columns were one-hot encoded \n",
    "\n",
    "Run the following cells to import the train, validate, and test sets:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all numeric features\n",
    "X_train_numeric = pd.read_csv('data/X_train_numeric.csv')\n",
    "X_val_numeric = pd.read_csv('data/X_val_numeric.csv')\n",
    "X_test_numeric = pd.read_csv('data/X_test_numeric.csv')\n",
    "\n",
    "# Load all categorical features\n",
    "X_train_cat = pd.read_csv('data/X_train_cat.csv')\n",
    "X_val_cat = pd.read_csv('data/X_val_cat.csv')\n",
    "X_test_cat = pd.read_csv('data/X_test_cat.csv')\n",
    "\n",
    "# Load all targets\n",
    "y_train = pd.read_csv('data/y_train.csv')\n",
    "y_val = pd.read_csv('data/y_val.csv')\n",
    "y_test = pd.read_csv('data/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features\n",
    "X_train = pd.concat([X_train_numeric, X_train_cat], axis=1)\n",
    "X_val = pd.concat([X_val_numeric, X_val_cat], axis=1)\n",
    "X_test = pd.concat([X_test_numeric, X_test_cat], axis=1)\n",
    "\n",
    "# Number of features\n",
    "n_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a refresher, preview the training data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>21453.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1969.0</td>\n",
       "      <td>1969.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>938.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>12420.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9742.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>5389.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>11003.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>765.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 296 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0        80.0         69.0  21453.0          6.0          5.0     1969.0   \n",
       "1        60.0         79.0  12420.0          7.0          5.0     2001.0   \n",
       "2        20.0         75.0   9742.0          8.0          5.0     2002.0   \n",
       "3       120.0         39.0   5389.0          8.0          5.0     1995.0   \n",
       "4        60.0         85.0  11003.0         10.0          5.0     2008.0   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  SaleType_ConLw  \\\n",
       "0        1969.0         0.0       938.0         0.0  ...             0.0   \n",
       "1        2001.0         0.0       666.0         0.0  ...             0.0   \n",
       "2        2002.0       281.0         0.0         0.0  ...             0.0   \n",
       "3        1996.0         0.0      1180.0         0.0  ...             0.0   \n",
       "4        2008.0       160.0       765.0         0.0  ...             0.0   \n",
       "\n",
       "   SaleType_New  SaleType_Oth  SaleType_WD  SaleCondition_Abnorml  \\\n",
       "0           0.0           0.0          1.0                    0.0   \n",
       "1           0.0           0.0          1.0                    0.0   \n",
       "2           0.0           0.0          1.0                    0.0   \n",
       "3           0.0           0.0          1.0                    0.0   \n",
       "4           0.0           0.0          1.0                    0.0   \n",
       "\n",
       "   SaleCondition_AdjLand  SaleCondition_Alloca  SaleCondition_Family  \\\n",
       "0                    0.0                   0.0                   0.0   \n",
       "1                    0.0                   0.0                   0.0   \n",
       "2                    0.0                   0.0                   0.0   \n",
       "3                    0.0                   0.0                   0.0   \n",
       "4                    0.0                   0.0                   0.0   \n",
       "\n",
       "   SaleCondition_Normal  SaleCondition_Partial  \n",
       "0                   1.0                    0.0  \n",
       "1                   1.0                    0.0  \n",
       "2                   1.0                    0.0  \n",
       "3                   1.0                    0.0  \n",
       "4                   1.0                    0.0  \n",
       "\n",
       "[5 rows x 296 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the data\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Baseline Model\n",
    "\n",
    "Building a naive baseline model to compare performance against is a helpful reference point. From there, you can then observe the impact of various tunning procedures which will iteratively improve your model. So, let's do just that! \n",
    "\n",
    "In the cell below: \n",
    "\n",
    "- Add an input layer with `n_features` units \n",
    "- Add two hidden layers, one with 100 and the other with 50 units (make sure you use the `'relu'` activation function) \n",
    "- Add an output layer with 1 unit and `'linear'` activation \n",
    "- Compile and fit the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 39253426176.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 2/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 39253426176.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 3/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 4/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253417984.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 5/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 6/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253417984.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 7/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253426176.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 8/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 9/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 10/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 11/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 12/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253417984.0000 - mse: 39253417984.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 13/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253426176.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 14/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 15/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253426176.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 16/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 17/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 18/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253430272.0000 - mse: 39253430272.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 19/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253426176.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 20/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 21/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 22/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 39253426176.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 23/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253417984.0000 - mse: 39253417984.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 24/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 39253430272.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 25/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 26/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 27/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253426176.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 28/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 29/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 30/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 31/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253426176.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 32/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253417984.0000 - mse: 39253417984.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 33/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 39253413888.0000 - mse: 39253413888.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 34/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 35/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 36/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253426176.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 37/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 38/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 39/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 40/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253430272.0000 - mse: 39253430272.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 41/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253430272.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 42/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253426176.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 43/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 44/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 45/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 46/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 47/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 48/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 49/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 39253422080.0000 - mse: 39253417984.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 50/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 51/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253417984.0000 - mse: 39253417984.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 52/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253426176.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 53/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 39253417984.0000 - mse: 39253417984.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 54/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 55/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 56/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253430272.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 57/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 39253417984.0000 - mse: 39253417984.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 58/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253417984.0000 - mse: 39253417984.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 59/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 60/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253426176.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 61/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 62/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 63/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 64/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 65/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253426176.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 66/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 67/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 39253417984.0000 - mse: 39253417984.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 68/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253426176.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 69/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253426176.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 70/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 71/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 72/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 73/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 39253426176.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 74/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253430272.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 75/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 76/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 77/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 78/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 79/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 80/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253417984.0000 - mse: 39253417984.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 81/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 82/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 39253426176.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 83/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253417984.0000 - mse: 39253413888.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 84/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253417984.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 85/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253417984.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 86/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253426176.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 87/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 88/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253426176.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 89/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 90/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 91/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 92/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253426176.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 93/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 94/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 95/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 39253426176.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 96/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 97/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253430272.0000 - mse: 39253430272.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 98/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253426176.0000 - mse: 39253426176.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 99/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253422080.0000 - mse: 39253422080.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 100/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 39253417984.0000 - mse: 39253417984.0000 - val_loss: 36061007872.0000 - val_mse: 36061007872.0000\n",
      "Epoch 101/150\n",
      " 1/33 [..............................] - ETA: 0s - loss: 39227580416.0000 - mse: 39227580416.0000"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "baseline_model = Sequential()\n",
    "\n",
    "# Hidden layer with 100 units\n",
    "\n",
    "\n",
    "# Hidden layer with 50 units\n",
    "\n",
    "\n",
    "# Output layer\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "baseline_model.compile(optimizer='SGD', \n",
    "                       loss='mse', \n",
    "                       metrics=['mse'])\n",
    "\n",
    "# Train the model\n",
    "baseline_model.fit(X_train, \n",
    "                   y_train, \n",
    "                   batch_size=32, \n",
    "                   epochs=150, \n",
    "                   validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _**Notice this extremely problematic behavior: all the values for training and validation loss are \"nan\". This indicates that the algorithm did not converge. The first solution to this is to normalize the input. From there, if convergence is not achieved, normalizing the output may also be required.**_ \n",
    "\n",
    "## Normalize the Input Data \n",
    "\n",
    "It's now time to normalize the input data. In the cell below: \n",
    "\n",
    "- Assign the column names of all numeric columns to `numeric_columns` \n",
    "- Instantiate a `StandardScaler` \n",
    "- Fit and transform `X_train_numeric`. Make sure you convert the result into a DataFrame (use `numeric_columns` as the column names) \n",
    "- Transform validate and test sets (`X_val_numeric` and `X_test_numeric`), and convert these results into DataFrames as well \n",
    "- Use the provided to combine the scaled numerical and categorical features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric column names\n",
    "numeric_columns = ['col1', 'col2', 'col3'] \n",
    "\n",
    "# Instantiate StandardScaler\n",
    "ss_X = pd.concat([X_train_scaled, X_train_categorical], axis=1)\n",
    "\n",
    "# Fit and transform train data\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_numeric), columns=numeric_columns)\n",
    "\n",
    "\n",
    "# Transform validate and test data\n",
    "X_val_scaled = pd.DataFrame(scaler.transform(X_val_numeric), columns=numeric_columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test_numeric), columns=numeric_columns)\n",
    "\n",
    "\n",
    "# Combine the scaled numerical features and categorical features\n",
    "X_train = pd.concat([X_train_scaled, X_train_cat], axis=1)\n",
    "X_val = pd.concat([X_val_scaled, X_val_cat], axis=1)\n",
    "X_test = pd.concat([X_test_scaled, X_test_cat], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the following cell to compile a neural network model (with the same architecture as before): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with all normalized inputs\n",
    "np.random.seed(123)\n",
    "normalized_input_model = Sequential()\n",
    "normalized_input_model.add(layers.Dense(100, activation='relu', input_shape=(n_features,)))\n",
    "normalized_input_model.add(layers.Dense(50, activation='relu'))\n",
    "normalized_input_model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "normalized_input_model.compile(optimizer='SGD', \n",
    "                               loss='mse', \n",
    "                               metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below: \n",
    "- Train the `normalized_input_model` on normalized input (`X_train`) and output (`y_train`) \n",
    "- Set a batch size of 32 and train for 150 epochs \n",
    "- Specify the `validation_data` argument as `(X_val, y_val)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_input_model.fit(X_train, y_train, batch_size=32, epochs=150, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _**Note that you still haven't achieved convergence! From here, it's time to normalize the output data.**_\n",
    "\n",
    "## Normalizing the output\n",
    "\n",
    "Again, use `StandardScaler()` to: \n",
    "\n",
    "- Fit and transform `y_train` \n",
    "- Transform `y_val` and `y_test` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate StandardScaler\n",
    "ss_y = StandardScaler()\n",
    "\n",
    "# Fit and transform train labels\n",
    "y_train_scaled = ss.fit_transform(y_train)\n",
    "\n",
    "# Transform validate and test labels\n",
    "y_val_scaled = ss.transform(y_val)\n",
    "y_test_scaled = ss.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below: \n",
    "- Train the `normalized_model` on normalized input (`X_train`) and output (`y_train_scaled`) \n",
    "- Set a batch size of 32 and train for 150 epochs \n",
    "- Specify the `validation_data` as `(X_val, y_val_scaled)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with all normalized inputs and outputs\n",
    "np.random.seed(123)\n",
    "normalized_model = Sequential()\n",
    "normalized_model.add(layers.Dense(100, activation='relu', input_shape=(n_features,)))\n",
    "normalized_model.add(layers.Dense(50, activation='relu'))\n",
    "normalized_model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "normalized_model.compile(optimizer='SGD', \n",
    "                         loss='mse', \n",
    "                         metrics=['mse']) \n",
    "\n",
    "# Train the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicely done! After normalizing both the input and output, the model finally converged. \n",
    "\n",
    "- Evaluate the model (`normalized_model`) on training data (`X_train` and `y_train_scaled`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on training data\n",
    "train_score = normalized_model.score(X_train, y_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate the model (`normalized_model`) on validate data (`X_val` and `y_val_scaled`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on validate data\n",
    "val_score = normalized_model.score(X_val, y_val_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the output is normalized, the metric above is not interpretable. To remedy this: \n",
    "\n",
    "- Generate predictions on validate data (`X_val`) \n",
    "- Transform these predictions back to original scale using `ss_y` \n",
    "- Now you can calculate the RMSE in the original units with `y_val` and `y_val_pred` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on validate data\n",
    "y_val_pred_scaled = normalized_model.predict(X_val)\n",
    "\n",
    "# Transform the predictions back to original scale\n",
    "y_val_pred = ss_y.inverse_transform(y_val_pred_scaled)\n",
    "\n",
    "# RMSE of validate data\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = mean_squared_error(y_val, y_val_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that you have a converged model, you can also experiment with alternative optimizers and initialization strategies to see if you can find a better global minimum. (After all, the current models may have converged to a local minimum.) \n",
    "\n",
    "## Using Weight Initializers\n",
    "\n",
    "In this section you will to use alternative initialization and optimization strategies. At the end, you'll then be asked to select the model which you believe performs the best.  \n",
    "\n",
    "##  He Initialization\n",
    "\n",
    "In the cell below, sepcify the following in the first hidden layer:  \n",
    "  - 100 units \n",
    "  - `'relu'` activation \n",
    "  - `input_shape` \n",
    "  - `kernel_initializer='he_normal'`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "he_model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "\n",
    "\n",
    "# Add another hidden layer\n",
    "he_model.add(layers.Dense(50, activation='relu'))\n",
    "\n",
    "# Add an output layer\n",
    "he_model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "he_model.compile(optimizer='SGD', \n",
    "                 loss='mse', \n",
    "                 metrics=['mse'])\n",
    "\n",
    "# Train the model\n",
    "he_model.fit(X_train, \n",
    "             y_train_scaled, \n",
    "             batch_size=32, \n",
    "             epochs=150, \n",
    "             validation_data=(X_val, y_val_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model (`he_model`) on training data (`X_train` and `y_train_scaled`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model (`he_model`) on validate data (`X_train` and `y_train_scaled`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on validate data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecun Initialization \n",
    "\n",
    "In the cell below, sepcify the following in the first hidden layer:  \n",
    "  - 100 units \n",
    "  - `'relu'` activation \n",
    "  - `input_shape` \n",
    "  - `kernel_initializer='lecun_normal'` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "lecun_model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "\n",
    "\n",
    "# Add another hidden layer\n",
    "lecun_model.add(layers.Dense(50, activation='relu'))\n",
    "\n",
    "# Add an output layer\n",
    "lecun_model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "lecun_model.compile(optimizer='SGD', \n",
    "                    loss='mse', \n",
    "                    metrics=['mse'])\n",
    "\n",
    "# Train the model\n",
    "lecun_model.fit(X_train, \n",
    "                y_train_scaled, \n",
    "                batch_size=32, \n",
    "                epochs=150, \n",
    "                validation_data=(X_val, y_val_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model (`lecun_model`) on training data (`X_train` and `y_train_scaled`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model (`lecun_model`) on validate data (`X_train` and `y_train_scaled`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on validate data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much of a difference, but a useful note to consider when tuning your network. Next, let's investigate the impact of various optimization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop \n",
    "\n",
    "Compile the `rmsprop_model` with: \n",
    "\n",
    "- `'rmsprop'` as the optimizer \n",
    "- track `'mse'` as the loss and metric  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "rmsprop_model = Sequential()\n",
    "rmsprop_model.add(layers.Dense(100, activation='relu', input_shape=(n_features,)))\n",
    "rmsprop_model.add(layers.Dense(50, activation='relu'))\n",
    "rmsprop_model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "\n",
    "# Train the model\n",
    "rmsprop_model.fit(X_train, \n",
    "                  y_train_scaled, \n",
    "                  batch_size=32, \n",
    "                  epochs=150, \n",
    "                  validation_data=(X_val, y_val_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model (`rmsprop_model`) on training data (`X_train` and `y_train_scaled`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model (`rmsprop_model`) on training data (`X_train` and `y_train_scaled`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on validate data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam \n",
    "\n",
    "Compile the `adam_model` with: \n",
    "\n",
    "- `'Adam'` as the optimizer \n",
    "- track `'mse'` as the loss and metric  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "adam_model = Sequential()\n",
    "adam_model.add(layers.Dense(100, activation='relu', input_shape=(n_features,)))\n",
    "adam_model.add(layers.Dense(50, activation='relu'))\n",
    "adam_model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "\n",
    "# Train the model\n",
    "adam_model.fit(X_train, \n",
    "               y_train_scaled, \n",
    "               batch_size=32, \n",
    "               epochs=150, \n",
    "               validation_data=(X_val, y_val_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model (`adam_model`) on training data (`X_train` and `y_train_scaled`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model (`adam_model`) on training data (`X_train` and `y_train_scaled`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on validate data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a Final Model\n",
    "\n",
    "Now, select the model with the best performance based on the training and validation sets. Evaluate this top model using the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As earlier, this metric is hard to interpret because the output is scaled. \n",
    "\n",
    "- Generate predictions on test data (`X_test`) \n",
    "- Transform these predictions back to original scale using `ss_y` \n",
    "- Now you can calculate the RMSE in the original units with `y_test` and `y_test_pred` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on test data\n",
    "y_test_pred_scaled = None\n",
    "\n",
    "# Transform the predictions back to original scale\n",
    "y_test_pred = None\n",
    "\n",
    "# MSE of test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lab, you worked to ensure your model converged properly by normalizing both the input and output. Additionally, you also investigated the impact of varying initialization and optimization routines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
