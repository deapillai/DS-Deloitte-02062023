{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Neural Networks with Regularization - Lab \n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you'll use a train-test partition as well as a validation set to get better insights about how to tune neural networks using regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. From there, you'll define and compile the model like before. \n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "- Apply early stopping criteria with a neural network \n",
    "- Apply L1, L2, and dropout regularization on a neural network  \n",
    "- Examine the effects of training with more data on a neural network  \n",
    "\n",
    "\n",
    "## Load the Data\n",
    "\n",
    "Run the following cell to import some of the libraries and classes you'll need in this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored in the file `'Bank_complaints.csv'`. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Product                       Consumer complaint narrative\n",
      "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
      "1  Student loan  I am being contacted by a debt collector for p...\n",
      "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
      "3  Student loan  Navient has sytematically and illegally failed...\n",
      "4  Student loan  My wife became eligible for XXXX Loan Forgiven...\n"
     ]
    }
   ],
   "source": [
    "# Load and preview the dataset\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before you begin to practice some of your new tools such as regularization and optimization, let's practice munging some data as you did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* Train - test split\n",
    "* One-hot encoding your complaint text\n",
    "* Transforming your category labels \n",
    "\n",
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since you have quite a bit of data and training neural networks takes a substantial amount of time and resources, downsample in order to test your initial pipeline. Going forward, these can be interesting areas of investigation: how does your model's performance change as you increase (or decrease) the size of your dataset?  \n",
    "\n",
    "- Generate a random sample of 10,000 observations using seed 123 for consistency of results. \n",
    "- Split this sample into `X` and `y` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataFrame' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m123\u001b[39m)\n\u001b[1;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBank_complaints.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m df_sample \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Split the data into X and y\u001b[39;00m\n\u001b[1;32m      7\u001b[0m X \u001b[38;5;241m=\u001b[39m df_sample\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_variable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataFrame' object is not callable"
     ]
    }
   ],
   "source": [
    "# Downsample the data\n",
    "np.random.seed(123)\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df_sample = df(n=10000)\n",
    "\n",
    "# Split the data into X and y\n",
    "X = df_sample.drop(columns=['target_variable'])\n",
    "y = df_sample['target_variable']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "\n",
    "- Split the data into training and test sets \n",
    "- Assign 1500 obervations to the test set and use 42 as the seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Split data into training and test sets\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mX\u001b[49m, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1500\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(data), random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1500/len(data), random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set \n",
    "\n",
    "As mentioned in the previous lesson, it is good practice to set aside a validation set, which is then used during hyperparameter tuning. Afterwards, when you have decided upon a final model, the test set can then be used to determine an unbiased perforance of the model. \n",
    "\n",
    "Run the cell below to further divide the training data into training and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Split the data into training and validation sets\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_train_final, X_val, y_train_final, y_val \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mX_train\u001b[49m, y_train, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train, test_size=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding the Complaints\n",
    "\n",
    "As before, you need to do some preprocessing before building a neural network model. \n",
    "\n",
    "- Keep the 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors \n",
    "- Transform the training, validate, and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use one-hot encoding to reformat the complaints into a matrix of vectors \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Only keep the 2000 most common words \u001b[39;00m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(num_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mfit_on_texts(\u001b[43mX_train\u001b[49m)\n\u001b[1;32m      7\u001b[0m X_train_tokens \u001b[38;5;241m=\u001b[39m  tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_matrix(X_train, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m X_val_tokens \u001b[38;5;241m=\u001b[39m  tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_matrix(X_val, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Use one-hot encoding to reformat the complaints into a matrix of vectors \n",
    "# Only keep the 2000 most common words \n",
    "\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_tokens =  tokenizer.texts_to_matrix(X_train, mode='binary')\n",
    "X_val_tokens =  tokenizer.texts_to_matrix(X_val, mode='binary')\n",
    "X_test_tokens = tokenizer.texts_to_matrix(X_test, mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "> **Note**: This is similar to your previous work with dummy variables. Each of the various product categories will be its own column, and each observation will be a row. In turn, each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero. \n",
    "\n",
    "Transform the training, validate, and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LabelEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Transform the product labels to numerical values\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m le \u001b[38;5;241m=\u001b[39m \u001b[43mLabelEncoder\u001b[49m()\n\u001b[1;32m      3\u001b[0m le\u001b[38;5;241m.\u001b[39mfit(labels)\n\u001b[1;32m      5\u001b[0m lb \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39mtransform(labels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LabelEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "# Transform the product labels to numerical values\n",
    "le = LabelEncoder()\n",
    "le.fit(labels)\n",
    "\n",
    "lb = le.transform(labels)\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(lb.reshape(-1, 1))\n",
    "\n",
    "y_train_lb = ohe.transform(y_train.reshape(-1, 1))\n",
    "y_val_lb = ohe.transform(y_val.reshape(-1, 1))\n",
    "y_test_lb = ohe.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Baseline Model \n",
    "\n",
    "Rebuild a fully connected (Dense) layer network:  \n",
    "- Use 2 hidden layers with 50 units in the first and 25 in the second layer, both with `'relu'` activation functions (since you are dealing with a multiclass problem, classifying the complaints into 7 classes) \n",
    "- Use a `'softmax'` activation function for the output layer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[0;32m----> 5\u001b[0m baseline_model \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m()\n\u001b[1;32m      6\u001b[0m baseline_model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m50\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(input_dim,)))\n\u001b[1;32m      7\u001b[0m baseline_model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m25\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "# Build a baseline neural network model using Keras\n",
    "random.seed(123)\n",
    "from keras import models\n",
    "from keras import layers\n",
    "baseline_model = Sequential()\n",
    "baseline_model.add(Dense(50, activation='relu', input_shape=(input_dim,)))\n",
    "baseline_model.add(Dense(25, activation='relu'))\n",
    "baseline_model.add(Dense(output_dim, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model\n",
    "\n",
    "Compile this model with: \n",
    "\n",
    "- a stochastic gradient descent optimizer \n",
    "- `'categorical_crossentropy'` as the loss function \n",
    "- a focus on `'accuracy'` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=SGD(), loss=categorical_crossentropy, metrics=[Accuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "- Train the model for 150 epochs in mini-batches of 256 samples \n",
    "- Include the `validation_data` argument to ensure you keep track of the validation loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "baseline_model_val = model.fit(train_data, train_labels, batch_size=256, epochs=150, validation_data=(val_data, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance\n",
    "\n",
    "The attribute `.history` (stored as a dictionary) contains four entries now: one per metric that was being monitored during training and validation. Print the keys of this dictionary for confirmation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dict_keys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Access the history attribute and store the dictionary\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m baseline_model_val_dict \u001b[38;5;241m=\u001b[39m \u001b[43mdict_keys\u001b[49m([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Print the keys\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(baseline_model_val_dict())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dict_keys' is not defined"
     ]
    }
   ],
   "source": [
    "# Access the history attribute and store the dictionary\n",
    "baseline_model_val_dict = dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
    "\n",
    "# Print the keys\n",
    "print(baseline_model_val_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate this model on the training data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m----------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mresults_train\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_train[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results_train' is not defined"
     ]
    }
   ],
   "source": [
    "print('----------')\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate this model on the test data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2311444795.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    results_test =\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "results_test = \n",
    "print('----------')\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Results \n",
    "\n",
    "Plot the loss versus the number of epochs. Be sure to include the training and the validation loss in the same plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Loss vs number of epochs with train and validation sets\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(model\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Loss vs number of epochs with train and validation sets\n",
    "plt.plot(model.history['loss'])\n",
    "plt.plot(model.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a second plot comparing training and validation accuracy to the number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Accuracy vs number of epochs with train and validation sets\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_acc\u001b[49m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(epochs, train_acc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbo\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining acc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(epochs, val_acc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation acc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_acc' is not defined"
     ]
    }
   ],
   "source": [
    "# Accuracy vs number of epochs with train and validation sets\n",
    "epochs = range(1, len(train_acc) + 1)\n",
    "\n",
    "plt.plot(epochs, train_acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice an interesting pattern here? Although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss don't necessarily do the same. After a certain point, validation accuracy keeps swinging, which means that you're probably **overfitting** the model to the training data when you train for many epochs past a certain dropoff point. Let's tackle this now. You will now specify an early stopping point when training your model. \n",
    "\n",
    "\n",
    "## Early Stopping\n",
    "\n",
    "Overfitting neural networks is something you **_want_** to avoid at all costs. However, it's not possible to know in advance how many *epochs* you need to train your model on, and running the model multiple times with varying number of *epochs* maybe helpful, but is a time-consuming process. \n",
    "\n",
    "We've defined a model with the same architecture as above. This time specify an early stopping point when training the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "model_2 = models.Sequential()\n",
    "model_2.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "model_2.add(layers.Dense(25, activation='relu'))\n",
    "model_2.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model_2.compile(optimizer='SGD', \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import `EarlyStopping` and `ModelCheckpoint` from `keras.callbacks` \n",
    "- Define a list, `early_stopping`: \n",
    "  - Monitor `'val_loss'` and continue training for 10 epochs before stopping \n",
    "  - Save the best model while monitoring `'val_loss'` \n",
    " \n",
    "> If you need help, consult [documentation](https://keras.io/callbacks/).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'early_stopping' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import EarlyStopping and ModelCheckpoint\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define the callbacks\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m \u001b[43mearly_stopping\u001b[49m(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'early_stopping' is not defined"
     ]
    }
   ],
   "source": [
    "# Import EarlyStopping and ModelCheckpoint\n",
    "\n",
    "\n",
    "# Define the callbacks\n",
    "early_stopping = early_stopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train `model_2`. Make sure you set the `callbacks` argument to `early_stopping`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_2_val \u001b[38;5;241m=\u001b[39m history \u001b[38;5;241m=\u001b[39m model_2\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m      2\u001b[0m validation_data\u001b[38;5;241m=\u001b[39mvalidation_data,\n\u001b[1;32m      3\u001b[0m callbacks\u001b[38;5;241m=\u001b[39m[early_stopping])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "model_2_val = history = model_2.fit(X_train, y_train, epochs=100, batch_size=32,\n",
    "validation_data=validation_data,\n",
    "callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best (saved) model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the best (saved) model\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m saved_model \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the best (saved) model\n",
    "\n",
    "saved_model = load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use this model to to calculate the training and test accuracy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'saved_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results_train \u001b[38;5;241m=\u001b[39m \u001b[43msaved_model\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(X_train_tokens, y_train_lb)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_train[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_train[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m----------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'saved_model' is not defined"
     ]
    }
   ],
   "source": [
    "results_train = saved_model.evaluate(X_train_tokens, y_train_lb)\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')\n",
    "\n",
    "print('----------')\n",
    "\n",
    "results_test = saved_model.evaluate(X_test_tokens, y_test_lb)\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicely done! Did you notice that the model didn't train for all 150 epochs? You reduced your training time. \n",
    "\n",
    "Now, take a look at how regularization techniques can further improve your model performance. \n",
    "\n",
    "## L2 Regularization \n",
    "\n",
    "First, take a look at L2 regularization. Keras makes L2 regularization easy. Simply add the `kernel_regularizer=keras.regularizers.l2(lambda_coeff)` parameter to any model layer. The `lambda_coeff` parameter determines the strength of the regularization you wish to perform. \n",
    "\n",
    "- Use 2 hidden layers with 50 units in the first and 25 in the second layer, both with `'relu'` activation functions \n",
    "- Add L2 regularization to both the hidden layers with 0.005 as the `lambda_coeff` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 21\u001b[0m\n\u001b[1;32m     16\u001b[0m L2_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSGD\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     17\u001b[0m                  loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     18\u001b[0m                  metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Train the model \u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m L2_model_val \u001b[38;5;241m=\u001b[39m L2_model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train_tokens\u001b[49m, \n\u001b[1;32m     22\u001b[0m                             y_train_lb, \n\u001b[1;32m     23\u001b[0m                             epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, \n\u001b[1;32m     24\u001b[0m                             batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, \n\u001b[1;32m     25\u001b[0m                             validation_data\u001b[38;5;241m=\u001b[39m(X_val_tokens, y_val_lb))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "# Import regularizers\n",
    "\n",
    "random.seed(123)\n",
    "L2_model = models.Sequential()\n",
    "\n",
    "# Add the input and first hidden layer\n",
    "\n",
    "\n",
    "# Add another hidden layer\n",
    "\n",
    "\n",
    "# Add an output layer\n",
    "L2_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "L2_model.compile(optimizer='SGD', \n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['acc'])\n",
    "\n",
    "# Train the model \n",
    "L2_model_val = L2_model.fit(X_train_tokens, \n",
    "                            y_train_lb, \n",
    "                            epochs=150, \n",
    "                            batch_size=256, \n",
    "                            validation_data=(X_val_tokens, y_val_lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, look at the training as well as the validation accuracy for both the L2 and the baseline models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'L2_model_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# L2 model details\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m L2_model_dict \u001b[38;5;241m=\u001b[39m \u001b[43mL2_model_val\u001b[49m\u001b[38;5;241m.\u001b[39mhistory\n\u001b[1;32m      3\u001b[0m L2_acc_values \u001b[38;5;241m=\u001b[39m L2_model_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m] \n\u001b[1;32m      4\u001b[0m L2_val_acc_values \u001b[38;5;241m=\u001b[39m L2_model_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'L2_model_val' is not defined"
     ]
    }
   ],
   "source": [
    "# L2 model details\n",
    "L2_model_dict = L2_model_val.history\n",
    "L2_acc_values = L2_model_dict['acc'] \n",
    "L2_val_acc_values = L2_model_dict['val_acc']\n",
    "\n",
    "# Baseline model\n",
    "baseline_model_acc = baseline_model_val_dict['acc'] \n",
    "baseline_model_val_acc = baseline_model_val_dict['val_acc']\n",
    "\n",
    "# Plot the accuracy for these models\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "ax.plot(epochs, L2_acc_values, label='Training acc L2')\n",
    "ax.plot(epochs, L2_val_acc_values, label='Validation acc L2')\n",
    "ax.plot(epochs, baseline_model_acc, label='Training acc')\n",
    "ax.plot(epochs, baseline_model_val_acc, label='Validation acc')\n",
    "ax.set_title('Training & validation accuracy L2 vs regular')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. Notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better.  \n",
    "\n",
    "\n",
    "## L1 Regularization\n",
    "\n",
    "Now have a look at L1 regularization. Will this work better? \n",
    "\n",
    "- Use 2 hidden layers with 50 units in the first and 25 in the second layer, both with `'relu'` activation functions \n",
    "- Add L1 regularization to both the hidden layers with 0.005 as the `lambda_coeff` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m L1_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSGD\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     15\u001b[0m                  loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     16\u001b[0m                  metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Train the model \u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m L1_model_val \u001b[38;5;241m=\u001b[39m L1_model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train_tokens\u001b[49m, \n\u001b[1;32m     20\u001b[0m                             y_train_lb, \n\u001b[1;32m     21\u001b[0m                             epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, \n\u001b[1;32m     22\u001b[0m                             batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, \n\u001b[1;32m     23\u001b[0m                             validation_data\u001b[38;5;241m=\u001b[39m(X_val_tokens, y_val_lb))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "L1_model = models.Sequential()\n",
    "\n",
    "# Add the input and first hidden layer\n",
    "\n",
    "\n",
    "# Add a hidden layer\n",
    "\n",
    "\n",
    "# Add an output layer\n",
    "L1_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "L1_model.compile(optimizer='SGD', \n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['acc'])\n",
    "\n",
    "# Train the model \n",
    "L1_model_val = L1_model.fit(X_train_tokens, \n",
    "                            y_train_lb, \n",
    "                            epochs=150, \n",
    "                            batch_size=256, \n",
    "                            validation_data=(X_val_tokens, y_val_lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training as well as the validation accuracy for the L1 model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'L1_model_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m L1_model_dict \u001b[38;5;241m=\u001b[39m \u001b[43mL1_model_val\u001b[49m\u001b[38;5;241m.\u001b[39mhistory\n\u001b[1;32m      5\u001b[0m acc_values \u001b[38;5;241m=\u001b[39m L1_model_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m] \n\u001b[1;32m      6\u001b[0m val_acc_values \u001b[38;5;241m=\u001b[39m L1_model_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'L1_model_val' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "L1_model_dict = L1_model_val.history\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "ax.plot(epochs, acc_values, label='Training acc L1')\n",
    "ax.plot(epochs, val_acc_values, label='Validation acc L1')\n",
    "ax.set_title('Training & validation accuracy with L1 regularization')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the training and validation accuracy don't diverge as much as before. Unfortunately, the validation accuracy isn't still that good. Next, experiment with dropout regularization to see if it offers any advantages. \n",
    "\n",
    "\n",
    "## Dropout Regularization \n",
    "\n",
    "It's time to try another technique: applying dropout to layers. As discussed in the earlier lesson, this involves setting a certain proportion of units in each layer to zero. In the following cell: \n",
    "\n",
    "- Apply a dropout rate of 30% to the input layer \n",
    "- Add a first hidden layer with 50 units and `'relu'` activation \n",
    "- Apply a dropout rate of 30% to the first hidden layer \n",
    "- Add a second hidden layer with 25 units and `'relu'` activation \n",
    "- Apply a dropout rate of 30% to the second hidden layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 31\u001b[0m\n\u001b[1;32m     26\u001b[0m dropout_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSGD\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     27\u001b[0m                       loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     28\u001b[0m                       metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m dropout_model_val \u001b[38;5;241m=\u001b[39m dropout_model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train_tokens\u001b[49m, \n\u001b[1;32m     32\u001b[0m                                       y_train_lb, \n\u001b[1;32m     33\u001b[0m                                       epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, \n\u001b[1;32m     34\u001b[0m                                       batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, \n\u001b[1;32m     35\u001b[0m                                       validation_data\u001b[38;5;241m=\u001b[39m(X_val_tokens, y_val_lb))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "#  This cell may take about a minute to run\n",
    "random.seed(123)\n",
    "dropout_model = models.Sequential()\n",
    "\n",
    "# Implement dropout to the input layer\n",
    "# NOTE: This is where you define the number of units in the input layer\n",
    "\n",
    "\n",
    "# Add the first hidden layer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Add the output layer\n",
    "dropout_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "dropout_model.compile(optimizer='SGD', \n",
    "                      loss='categorical_crossentropy', \n",
    "                      metrics=['acc'])\n",
    "\n",
    "# Train the model\n",
    "dropout_model_val = dropout_model.fit(X_train_tokens, \n",
    "                                      y_train_lb, \n",
    "                                      epochs=150, \n",
    "                                      batch_size=256, \n",
    "                                      validation_data=(X_val_tokens, y_val_lb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results_train \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(X_train_tokens, y_train_lb)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_train[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_train[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m----------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(X_train_tokens, y_train_lb)\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')\n",
    "\n",
    "print('----------')\n",
    "\n",
    "results_test = model.evaluate(X_test_tokens, y_test_lb)\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again, and the training and test accuracy are very close!  \n",
    "\n",
    "## Bigger Data? \n",
    "\n",
    "Finally, let's examine if we can improve the model's performance just by adding more data. We've quadrapled the sample dataset from 10,000 to 40,000 observations, and all you need to do is run the code! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bigger_sample = df.sample(40000, random_state=123)\n",
    "\n",
    "X = df['Consumer complaint narrative']\n",
    "y = df['Product']\n",
    "\n",
    "# Train-test split\n",
    "X_train_bigger, X_test_bigger, y_train_bigger, y_test_bigger = train_test_split(X, \n",
    "                                                                                y, \n",
    "                                                                                test_size=6000, \n",
    "                                                                                random_state=42)\n",
    "\n",
    "# Validation set\n",
    "X_train_final_bigger, X_val_bigger, y_train_final_bigger, y_val_bigger = train_test_split(X_train_bigger, \n",
    "                                                                                          y_train_bigger, \n",
    "                                                                                          test_size=4000, \n",
    "                                                                                          random_state=42)\n",
    "\n",
    "\n",
    "# One-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(X_train_final_bigger)\n",
    "\n",
    "X_train_tokens_bigger = tokenizer.texts_to_matrix(X_train_final_bigger, mode='binary')\n",
    "X_val_tokens_bigger = tokenizer.texts_to_matrix(X_val_bigger, mode='binary')\n",
    "X_test_tokens_bigger = tokenizer.texts_to_matrix(X_test_bigger, mode='binary')\n",
    "\n",
    "# One-hot encoding of products\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_train_final_bigger)\n",
    "\n",
    "y_train_lb_bigger = to_categorical(lb.transform(y_train_final_bigger))[:, :, 1]\n",
    "y_val_lb_bigger = to_categorical(lb.transform(y_val_bigger))[:, :, 1]\n",
    "y_test_lb_bigger = to_categorical(lb.transform(y_test_bigger))[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  This cell may take several minutes to run\n",
    "random.seed(123)\n",
    "bigger_data_model = models.Sequential()\n",
    "bigger_data_model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "bigger_data_model.add(layers.Dense(25, activation='relu'))\n",
    "bigger_data_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "bigger_data_model.compile(optimizer='SGD', \n",
    "                          loss='categorical_crossentropy', \n",
    "                          metrics=['acc'])\n",
    "\n",
    "bigger_data_model_val = bigger_data_model.fit(X_train_tokens_bigger,  \n",
    "                                              y_train_lb_bigger,  \n",
    "                                              epochs=150,  \n",
    "                                              batch_size=256,  \n",
    "                                              validation_data=(X_val_tokens_bigger, y_val_lb_bigger))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bigger_data_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results_train \u001b[38;5;241m=\u001b[39m \u001b[43mbigger_data_model\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(X_train_tokens_bigger, y_train_lb_bigger)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_train[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_train[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m----------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bigger_data_model' is not defined"
     ]
    }
   ],
   "source": [
    "results_train = bigger_data_model.evaluate(X_train_tokens_bigger, y_train_lb_bigger)\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')\n",
    "\n",
    "print('----------')\n",
    "\n",
    "results_test = bigger_data_model.evaluate(X_val_tokens_bigger, y_val_lb_bigger)\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs and no regularization technique, you were able to get both better test accuracy and loss. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance! \n",
    "\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database \n",
    "\n",
    "\n",
    "## Summary  \n",
    "\n",
    "In this lesson, you built deep learning models using a validation set and used several techniques such as L2 and L1 regularization, dropout regularization, and early stopping to improve the accuracy of your models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
